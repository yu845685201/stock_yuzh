"""
åŸºæœ¬é¢æ•°æ®é‡‡é›†ç®¡ç†å™¨ - å®ç”¨å¹³è¡¡æ–¹æ¡ˆå®ç°
ä¸¥æ ¼æŒ‰ç…§äº§å“è®¾è®¡æ–‡æ¡£è¦æ±‚å®ç°è‚¡ç¥¨åŸºæœ¬é¢ä¿¡æ¯é‡‡é›†åŠŸèƒ½
"""

import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

from ..config import ConfigManager
from ..database.connection import DatabaseConnection
from ..data_sources.baostock_source import BaostockSource
from ..sync.csv_writer import CsvWriter
from ..models.collection_result import CollectionResult, CollectionStatistics, CollectionStatus


class FundamentalsManager:
    """åŸºæœ¬é¢æ•°æ®é‡‡é›†ç®¡ç†å™¨ - å®ç”¨å¹³è¡¡æ–¹æ¡ˆ"""

    def __init__(self, config_manager: ConfigManager = None):
        """
        åˆå§‹åŒ–åŸºæœ¬é¢é‡‡é›†ç®¡ç†å™¨

        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
        """
        self.config = config_manager or ConfigManager()
        self.db = DatabaseConnection(self.config)
        self.baostock = BaostockSource(self.config.get('data_sources.baostock', {}))
        self.csv_writer = CsvWriter(self.config)
        self.logger = logging.getLogger(__name__)

    def execute_sync(self, **options) -> Dict[str, Any]:
        """
        æ‰§è¡ŒåŸºæœ¬é¢æ•°æ®åŒæ­¥

        Args:
            **options: é…ç½®é€‰é¡¹
                - batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤50
                - dry_run: æ˜¯å¦è¯•è¿è¡Œï¼Œé»˜è®¤False
                - list_status: è‚¡ç¥¨ä¸Šå¸‚çŠ¶æ€è¿‡æ»¤ï¼Œé»˜è®¤'L'ï¼ˆä»…ä¸Šå¸‚ï¼‰

        Returns:
            åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        """
        batch_size = options.get('batch_size', 50)
        dry_run = options.get('dry_run', False)
        list_status = options.get('list_status', 'L')

        # æ–°å¢ç»Ÿè®¡ç±»ç”¨äºæ›´ç²¾ç¡®çš„çŠ¶æ€ç®¡ç†
        collection_stats = CollectionStatistics()

        stats = {
            'total_stocks': 0,
            'successful': 0,      # ä¿æŒå‘åå…¼å®¹
            'failed': 0,          # ä¿æŒå‘åå…¼å®¹
            'no_data': 0,         # æ–°å¢ï¼šæ— æ•°æ®ç»Ÿè®¡
            'error_count': 0,     # æ–°å¢ï¼šå¼‚å¸¸å¤±è´¥ç»Ÿè®¡
            'csv_files': [],
            'start_time': datetime.now(),
            'batch_count': 0,
            'timing': {
                'baostock_total': 0.0,
                'csv_total': 0.0,
                'db_total': 0.0,
                'baostock_calls': 0,
                'csv_batches': 0,
                'db_batches': 0
            }
        }

        try:
            # è¿æ¥æ•°æ®æº
            if not self.baostock.connect():
                raise Exception("Baostockè¿æ¥å¤±è´¥")

            # è·å–è‚¡ç¥¨åˆ—è¡¨
            stocks = self._get_stock_list(list_status)
            stats['total_stocks'] = len(stocks)

            if not stocks:
                self.logger.warning(f"æ²¡æœ‰æ‰¾åˆ°ä¸Šå¸‚çŠ¶æ€ä¸º'{list_status}'çš„è‚¡ç¥¨")
                return stats

            self.logger.info(f"å¼€å§‹åŒæ­¥åŸºæœ¬é¢æ•°æ®: æ€»è®¡{stats['total_stocks']}åªè‚¡ç¥¨ï¼Œæ‰¹æ¬¡å¤§å°{batch_size}")

            # åˆ†æ‰¹å¤„ç†
            fundamentals_data = []
            for i, stock in enumerate(stocks):
                try:
                    # è®°å½•å•åªè‚¡ç¥¨å¤„ç†å¼€å§‹æ—¶é—´
                    stock_start_time = time.time()

                    # ä½¿ç”¨æ–°çš„é‡‡é›†æ–¹æ³•ï¼Œè¿”å›CollectionResult
                    collection_result = self._collect_fundamentals_with_status(stock)

                    # è®°å½•baostockè€—æ—¶
                    baostock_time = time.time() - stock_start_time
                    stats['timing']['baostock_total'] += baostock_time
                    stats['timing']['baostock_calls'] += 1

                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    collection_stats.add_result(collection_result)

                    # æ ¹æ®ç»“æœçŠ¶æ€å¤„ç†
                    if collection_result.is_success:
                        fundamentals_data.append(collection_result.data)
                        stats['successful'] += 1

                        # è¾“å‡ºè¿›åº¦æ¡
                        progress_percent = (i + 1) / len(stocks) * 100
                        self.logger.info(f"[{i+1}/{len(stocks)}]({stock['stock_code']}-{stock['stock_name']}) "
                                         f"è¿›åº¦: {progress_percent:.1f}% baostock: {baostock_time:.3f}s")
                    elif collection_result.is_no_data:
                        stats['no_data'] += 1
                        self.logger.info(f"[{i+1}/{len(stocks)}]({stock['stock_code']}-{stock['stock_name']}) "
                                         f"è¿›åº¦: {((i+1)/len(stocks)*100):.1f}% æ— æ•°æ® baostock: {baostock_time:.3f}s")
                    else:  # ERROR
                        stats['error_count'] += 1
                        stats['failed'] += 1  # ä¿æŒå‘åå…¼å®¹
                        self.logger.error(f"[{i+1}/{len(stocks)}]({stock['stock_code']}-{stock['stock_name']}) "
                                         f"è¿›åº¦: {((i+1)/len(stocks)*100):.1f}% é‡‡é›†å¤±è´¥: {collection_result.error_message} baostock: {baostock_time:.3f}s")

                    # QPSæ§åˆ¶ï¼šæ¯å¤„ç†ä¸€åªè‚¡ç¥¨åçŸ­æš‚ä¼‘çœ ï¼Œç¡®ä¿QPSä¸è¶…è¿‡50
                    if not dry_run and i < len(stocks) - 1:
                        time.sleep(0.02)  # 1/50ç§’ = 0.02ç§’

                    # æ‰¹å¤„ç†ï¼šæ¯batch_sizeåªè‚¡ç¥¨å¤„ç†ä¸€æ¬¡æ•°æ®åº“å†™å…¥
                    if (i + 1) % batch_size == 0:
                        if fundamentals_data:
                            self._process_data_batch(fundamentals_data, dry_run, stats)
                            stats['batch_count'] += 1
                            fundamentals_data = []

                        # é¢å¤–çš„æ‰¹æ¬¡é—´ä¼‘çœ ï¼ˆå¯é€‰ï¼Œæä¾›æ›´ä¿å®ˆçš„QPSæ§åˆ¶ï¼‰
                        if i < len(stocks) - 1:
                            self.logger.debug(f"å·²å®Œæˆæ‰¹æ¬¡ {((i+1)//batch_size)}ï¼Œä¼‘çœ 0.5ç§’...")
                            time.sleep(0.5)

                except Exception as e:
                    # åˆ›å»ºé”™è¯¯ç»“æœå¹¶æ›´æ–°ç»Ÿè®¡
                    error_result = CollectionResult.error(str(e), time.time() - stock_start_time)
                    collection_stats.add_result(error_result)
                    stats['error_count'] += 1
                    stats['failed'] += 1  # ä¿æŒå‘åå…¼å®¹
                    self.logger.error(f"[{i+1}/{len(stocks)}]({stock['stock_code']}-{stock['stock_name']}) "
                                     f"å¤„ç†å¼‚å¸¸: {e}")

            # å¤„ç†å‰©ä½™æ•°æ®
            if fundamentals_data:
                self._process_data_batch(fundamentals_data, dry_run, stats)
                stats['batch_count'] += 1

            stats['end_time'] = datetime.now()
            stats['duration'] = (stats['end_time'] - stats['start_time']).total_seconds()

            # æ›´æ–°å‘åå…¼å®¹çš„å­—æ®µ
            stats['failed'] = stats['error_count']  # failedç°åœ¨åªåŒ…å«çœŸæ­£çš„å¼‚å¸¸å¤±è´¥

            # è®¡ç®—å„ç§æˆåŠŸç‡
            stats['success_rate'] = collection_stats.real_success_rate  # çœŸå®æˆåŠŸç‡
            stats['completion_rate'] = collection_stats.completion_rate  # å®Œæˆç‡ï¼ˆåŒ…å«æ— æ•°æ®ï¼‰
            stats['error_rate'] = collection_stats.error_rate  # é”™è¯¯ç‡

            # æ·»åŠ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            stats['collection_stats'] = collection_stats.to_dict()

            # è¾“å‡ºè¯¦ç»†çš„è€—æ—¶ç»Ÿè®¡
            self._log_timing_summary(stats, collection_stats)

            return stats

        except Exception as e:
            self.logger.error(f"åŸºæœ¬é¢æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats['error'] = str(e)
            return stats
        finally:
            self.baostock.disconnect()

    def _log_timing_summary(self, stats: Dict[str, Any], collection_stats: CollectionStatistics = None) -> None:
        """
        è¾“å‡ºè¯¦ç»†çš„è€—æ—¶ç»Ÿè®¡ä¿¡æ¯

        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
            collection_stats: é‡‡é›†ç»Ÿè®¡å¯¹è±¡
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š åŸºæœ¬é¢æ•°æ®åŒæ­¥å®Œæˆç»Ÿè®¡")
        self.logger.info("=" * 60)

        # ä¸‰åˆ†ç±»ç»Ÿè®¡ï¼ˆæ–°åŠŸèƒ½ï¼‰
        if collection_stats:
            self.logger.info("ğŸ“‹ ä¸‰åˆ†ç±»ç»Ÿè®¡:")
            self.logger.info(f"   âœ… æˆåŠŸé‡‡é›†: {collection_stats.success_count} åªè‚¡ç¥¨")
            self.logger.info(f"   ğŸ“„ æ— æ•°æ®: {collection_stats.no_data_count} åªè‚¡ç¥¨")
            self.logger.info(f"   âŒ å¼‚å¸¸å¤±è´¥: {collection_stats.error_count} åªè‚¡ç¥¨")
            self.logger.info("")
            self.logger.info("ğŸ“ˆ å®Œæˆåº¦åˆ†æ:")
            self.logger.info(f"   ğŸ¯ çœŸå®æˆåŠŸç‡: {collection_stats.real_success_rate:.2%}")
            self.logger.info(f"   ğŸ“‹ å®Œæˆç‡: {collection_stats.completion_rate:.2%}")
            self.logger.info(f"   âš ï¸  é”™è¯¯ç‡: {collection_stats.error_rate:.2%}")
        else:
            # å‘åå…¼å®¹çš„ç»Ÿè®¡æ˜¾ç¤º
            self.logger.info(f"âœ… æˆåŠŸé‡‡é›†: {stats['successful']} åªè‚¡ç¥¨")
            self.logger.info(f"âŒ å¤±è´¥æ•°é‡: {stats['failed']} åªè‚¡ç¥¨")
            self.logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {stats.get('success_rate', 0):.2%}")

        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {stats.get('duration', 0):.2f} ç§’")

        # è¯¦ç»†è€—æ—¶åˆ†æ
        if 'timing' in stats:
            timing = stats['timing']

            self.logger.info("ğŸ“‹ è¯¦ç»†è€—æ—¶åˆ†æ:")

            # Baostock APIç»Ÿè®¡
            if timing['baostock_calls'] > 0:
                avg_baostock = timing['baostock_total'] / timing['baostock_calls']
                self.logger.info(f"   ğŸ” Baostock API: {timing['baostock_total']:.2f}s "
                                f"(è°ƒç”¨{timing['baostock_calls']}æ¬¡, å¹³å‡{avg_baostock:.3f}s/åª)")

            # CSVç”Ÿæˆç»Ÿè®¡
            if timing['csv_batches'] > 0:
                avg_csv = timing['csv_total'] / timing['csv_batches']
                self.logger.info(f"   ğŸ“„ CSVç”Ÿæˆ: {timing['csv_total']:.2f}s "
                                f"(å¤„ç†{timing['csv_batches']}æ‰¹æ¬¡, å¹³å‡{avg_csv:.3f}s/æ‰¹æ¬¡)")

            # æ•°æ®åº“å†™å…¥ç»Ÿè®¡
            if timing['db_batches'] > 0:
                avg_db = timing['db_total'] / timing['db_batches']
                self.logger.info(f"   ğŸ’¾ æ•°æ®åº“å†™å…¥: {timing['db_total']:.2f}s "
                                f"(å¤„ç†{timing['db_batches']}æ‰¹æ¬¡, å¹³å‡{avg_db:.3f}s/æ‰¹æ¬¡)")

            # æ•ˆç‡åˆ†æ
            if timing['baostock_calls'] > 0:
                stocks_per_second = timing['baostock_calls'] / timing['baostock_total']
                self.logger.info(f"   âš¡ å¤„ç†æ•ˆç‡: {stocks_per_second:.1f} åªè‚¡ç¥¨/ç§’")

        self.logger.info("=" * 60)

    def _get_stock_list(self, list_status: str = 'L') -> List[Dict[str, Any]]:
        """
        è·å–æœªé€€å¸‚è‚¡ç¥¨åˆ—è¡¨

        Args:
            list_status: ä¸Šå¸‚çŠ¶æ€è¿‡æ»¤ï¼Œ'L'=ä¸Šå¸‚ï¼Œ'D'=é€€å¸‚ï¼Œ'P'=æš‚åœä¸Šå¸‚

        Returns:
            è‚¡ç¥¨åˆ—è¡¨
        """
        query = """
            SELECT ts_code, stock_code, stock_name
            FROM base_stock_info
            WHERE list_status = %s
            ORDER BY ts_code
        """
        return self.db.execute_query(query, (list_status,))

    def _collect_fundamentals(self, stock: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        é‡‡é›†å•åªè‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰

        Args:
            stock: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

        Returns:
            åŸºæœ¬é¢æ•°æ®æˆ–None
        """
        result = self._collect_fundamentals_with_status(stock)
        return result.get_data_or_none()

    def _collect_fundamentals_with_status(self, stock: Dict[str, Any]) -> CollectionResult:
        """
        é‡‡é›†å•åªè‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ï¼Œè¿”å›å¸¦çŠ¶æ€çš„ç»“æœ

        Args:
            stock: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

        Returns:
            CollectionResult: åŒ…å«çŠ¶æ€ã€æ•°æ®å’Œé”™è¯¯ä¿¡æ¯çš„ç»“æœå¯¹è±¡
        """
        start_time = time.time()
        try:
            # ä½¿ç”¨ç°æœ‰çš„baostockæ–¹æ³•è·å–åŸºæœ¬é¢æ•°æ®
            fundamentals = self.baostock.get_stock_fundamentals(stock['ts_code'])

            execution_time = time.time() - start_time

            if fundamentals:
                # ç¡®ä¿åŒ…å«è‚¡ç¥¨åç§°
                fundamentals['stock_name'] = stock['stock_name']
                self.logger.debug(f"æˆåŠŸè·å–è‚¡ç¥¨ {stock['ts_code']} åŸºæœ¬é¢æ•°æ®")
                return CollectionResult.success(fundamentals, execution_time)
            else:
                # æ— æ•°æ®æƒ…å†µï¼Œæ¥å£æ­£å¸¸ä½†è¿”å›ç©ºç»“æœ
                self.logger.debug(f"è‚¡ç¥¨ {stock['ts_code']} æ— åŸºæœ¬é¢æ•°æ®")
                return CollectionResult.no_data(execution_time)

        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.debug(f"é‡‡é›†è‚¡ç¥¨ {stock['ts_code']} åŸºæœ¬é¢æ•°æ®å¼‚å¸¸: {e}")
            return CollectionResult.error(str(e), execution_time)

    def _process_data_batch(self, batch_data: List[Dict[str, Any]], dry_run: bool = False, stats: Dict[str, Any] = None):
        """
        å¤„ç†æ•°æ®æ‰¹æ¬¡

        Args:
            batch_data: æ‰¹é‡æ•°æ®
            dry_run: æ˜¯å¦è¯•è¿è¡Œ
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼Œç”¨äºè®°å½•è€—æ—¶
        """
        if not batch_data:
            return

        if not dry_run:
            try:
                # è®°å½•CSVç”Ÿæˆå¼€å§‹æ—¶é—´
                csv_start_time = time.time()

                # å†™å…¥CSVæ–‡ä»¶
                self.csv_writer.write_base_fundamentals_info(batch_data)

                # è®°å½•CSVç”Ÿæˆè€—æ—¶
                csv_time = time.time() - csv_start_time
                if stats and 'timing' in stats:
                    stats['timing']['csv_total'] += csv_time
                    stats['timing']['csv_batches'] += 1

                # è®°å½•æ•°æ®åº“å†™å…¥å¼€å§‹æ—¶é—´
                db_start_time = time.time()

                # æ•°æ®åº“upsert
                affected_rows = self.db.upsert_fundamentals_data(batch_data)

                # è®°å½•æ•°æ®åº“å†™å…¥è€—æ—¶
                db_time = time.time() - db_start_time
                if stats and 'timing' in stats:
                    stats['timing']['db_total'] += db_time
                    stats['timing']['db_batches'] += 1

                self.logger.info(f"æ‰¹æ¬¡å¤„ç†å®Œæˆ: {len(batch_data)} æ¡è®°å½•ï¼ŒCSV: {csv_time:.3f}sï¼ŒDB: {db_time:.3f}sï¼Œå½±å“è¡Œæ•°: {affected_rows}")

            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                raise
        else:
            self.logger.info(f"DRY RUN: å°†å¤„ç† {len(batch_data)} æ¡è®°å½•")


def sync_fundamentals_data(config_manager: ConfigManager = None, **options) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŒæ­¥åŸºæœ¬é¢æ•°æ®

    Args:
        config_manager: é…ç½®ç®¡ç†å™¨
        **options: åŒæ­¥é€‰é¡¹

    Returns:
        åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
    """
    manager = FundamentalsManager(config_manager)
    return manager.execute_sync(**options)